{"cells":[{"metadata":{},"cell_type":"markdown","source":"# import"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nimport re\n\nfrom collections import Counter\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn import preprocessing\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\npd.set_option('display.max_colwidth', -1)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['URL_TA']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки."},{"metadata":{},"cell_type":"markdown","source":"# 2. Обработка признаков\n## 2.1 Обработка ресторан ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"def edit_rest_id(df_input):    \n    # full edit restaurant_id \n    \n    df = df_input.copy()\n    \n    # создания признака один ли ресторан или это сеть\n    df['Restaurant_id'] = df['Restaurant_id'].apply(lambda x: int(x[3:]))\n    chain_restaurant = df.groupby('Restaurant_id')['City'].count().rename('one_or_more')\n    df = pd.merge(df, chain_restaurant, on='Restaurant_id')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edit_rest_id(data).sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Обработка City"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['City'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Дополнительным признаком можно добавить население. Городов не очень много, поэтому буду просто гуглить. Мне понравился сайт https://datacommons.org/\nБуду считать что население не сильно изменилось за пару лет, поэтому не буду обращать внимание на год, за который представлены данные населения"},{"metadata":{"trusted":true},"cell_type":"code","source":"# население для каждого города из датасета\npopulation = {\n    'Paris': 2160928,\n    'Helsinki': 631695,\n    'Edinburgh': 482005,\n    'London': 8982256,\n    'Bratislava': 424428,\n    'Lisbon': 504718,\n    'Budapest': 1756056,\n    'Stockholm': 789024,\n    'Rome': 2873494,\n    'Milan': 1351562,\n    'Munich': 1471508,\n    'Hamburg': 1841179,\n    'Prague': 1308632,\n    'Vienna': 1897491,\n    'Dublin': 544107,\n    'Barcelona': 1620343,\n    'Brussels': 174383,\n    'Madrid': 3223334,\n    'Oslo': 634293,\n    'Amsterdam': 869709,\n    'Berlin': 3644826,\n    'Lyon': 513275,\n    'Athens': 664046,\n    'Warsaw': 1764615,\n    'Oporto': 214349,\n    'Krakow': 766683,\n    'Copenhagen': 602481,\n    'Luxembourg': 114303,\n    'Zurich': 402762,\n    'Geneva': 198979,\n    'Ljubljana': 279631              \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# принадлежность города к стране\ncountry = {\n    'Paris': 'France',\n    'Hamburg': 'Germany',\n    'Rome': 'Italy',\n    'London': 'UK',\n    'Milan': 'Italy',\n    'Madrid': 'Spain',\n    'Oslo': 'Norway',\n    'Stockholm': 'Sweden',\n    'Krakow': 'Poland',\n    'Lyon': 'Paris',\n    'Lisbon': 'Portugal',\n    'Edinburgh': 'UK',\n    'Vienna': 'Austria',\n    'Warsaw': 'Poland',\n    'Amsterdam': 'Netherlands',\n    'Budapest': 'Hungary',\n    'Helsinki': 'Finland',\n    'Zurich': 'Switzerland',\n    'Luxembourg': 'Luxembourg',\n    'Berlin': 'Germany',\n    'Prague': 'Czechia',\n    'Munich': 'Germany',\n    'Bratislava': 'Slovakia',\n    'Brussels': 'Belgium',\n    'Ljubljana': 'Slovenia',\n    'Copenhagen': 'Denmark',\n    'Oporto': 'Portugal',\n    'Barcelona': 'Spain',\n    'Geneva': 'Switzerland',\n    'Athens': 'Greece',\n    'Dublin': 'Ireland'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# вышло так, что городов не столиц оказалось меньше, поэтому что меньше писать составлю из них список\nnot_Capital = ['Barcelona', 'Milan', 'Hamburg', 'Munich','Lyon', 'Zurich', 'Oporto', 'Geneva', 'Krakow']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dummy(df_input, series, n=10):\n    # one hot encoding for n popular elements\n    df = df_input.copy()    \n    # list of n top elements\n    top_elements = df[series].value_counts().index[:n]\n    \n    df[series] = df[series].apply(lambda x: x if x in top_elements else 'other')\n    \n    # create dummy for elements in top_elements\n    dummy = pd.get_dummies(df[series])\n    df = pd.concat([df, dummy], axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def edit_city(df_input):\n    # full edit city and about city\n    \n    df = df_input.copy()\n    scaler = StandardScaler()\n    \n    # население каждого города\n    df['population'] = df['City'].map(population)\n\n    # Определение страны для каждого города из датасета\n    df['country'] = df['City'].map(country)\n\n    # сколько ресторанов в каждом городе\n    restaurant_in_city = df.groupby('City')['Reviews'].count().rename('restaurant_in_city')\n    df = pd.merge(df, restaurant_in_city, on='City')\n\n    # количество ресторанов на каждого человека\n    df['restaurant_per_person'] = df['restaurant_in_city'] / df['population']\n    df['restaurant_per_person'] = scaler.fit_transform(df[['restaurant_per_person']])\n\n    # столица или не столица\n    df['capital'] = df['City'].apply(lambda x: 0 if x in not_Capital else 1 )\n\n    return df    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edit_city(data).sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Возможно надо подумать над признаком, туристический ли это город или нет. Или сколько денег приносит в бюджет города, но пока что не знаю как это реализовать.\n\nБыли создананы следующие признаки:\n- население каждого города\n- принадлежность города к стране\n- количество ресторанов для каждого города\n- из этого можно получить количество ресторанов на каждого человека\n- является ли город столицей \n- dummy переменные (видел в других нотбуках label endocting, но не знаю зачем его применять к этому признаку)"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Кухни"},{"metadata":{"trusted":true},"cell_type":"code","source":"def only_top_cuisins(string):\n#     возвращает только популярные кухни\n    output = []\n\n    string_to_list = re.findall(r\"'(\\b.*?\\b)'\", string)\n    # проверка для каждой входит ли в топ\n    for cuisin in string_to_list:\n        if cuisin in top_cuisins:\n            output.append(cuisin)\n    if not output:\n        return ['Other']\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def edit_cuisine(input_df, n=10):\n    # edit all about cuisine style\n    \n    df = input_df.copy()\n    global top_cuisins\n    \n    # создаю столбец, с отметками о том, где были пропуски\n    df['Cuisine_style_ISNA'] = df['Cuisine Style'].isna().astype(int)\n\n    # заполнение пропусков\n    df['Cuisine Style'] = df['Cuisine Style'].fillna(\"['Other']\")\n    \n    # топ кухонь по популярности\n    # перебор список все списков видов кухни\n    all_cuisins = df['Cuisine Style'].str.findall(r\"'(\\b.*?\\b)'\")\n    cuisins = []\n    for list_cuisins in all_cuisins:\n        for style in list_cuisins:\n            cuisins.append(style)\n    # создание списка n популярных кухонь\n    set_cuisins = set(cuisins)        \n    cuisins = dict(Counter(cuisins))\n    cuisins = {keys: values for keys, values in sorted(cuisins.items(), reverse=True, key=lambda x: x[1])}\n    top_cuisins = list(cuisins.keys())[:n]\n    \n    # возвращаю только топ n кухонь, остальные other\n    df['Cuisine Style'] = df['Cuisine Style'].apply(lambda x: only_top_cuisins(x))\n\n    # разнообразие кухонь\n    df['len_cuisine'] = df['Cuisine Style'].apply(len) \n    \n    # созднаие dummy переменных для видов кухни\n    for cuisin in top_cuisins:\n        df[cuisin] = df['Cuisine Style'].apply(lambda x: 1 if cuisin in x else 0)   \n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edit_cuisine(data, n=5).sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В признаке достаточно много пропусков\n- создал столбец с пометками о пропусках\n- пропуски заменил значением other\n- создал признак разнообразия кухонь\n- создал функцию для создания n популярных dummy переменных"},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Price Range"},{"metadata":{"trusted":true},"cell_type":"code","source":"# хочу посмотреть самый популярный диапазон цен для каждого города\ncities = data['City'].unique()\n# отображение самых популярных диапазонов цен для каждого города\nfor city in cities:\n    print(data[data['City']==city]['Price Range'].value_counts().index[0], city)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def edit_price_range(input_df):\n    # edit all about price range\n    \n    df = input_df.copy()\n    \n    # перед обработкой признака, создаю колонку с отображениями пропуска\n    df['Price_Range_isNAN'] = df['Price Range'].isnull().astype(int)\n\n    # выглядит так, что пропуски можно заполнить через fillna самым популярным значением\n    df['Price Range'] = df['Price Range'].fillna('$$ - $$$')\n\n    # замена значений типа объект на циферки \n    replace_price_range = {'$': 0, '$$ - $$$': 1, '$$$$':2}\n    df['Price Range'] = df['Price Range'].map(replace_price_range)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edit_price_range(data).sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Никаких новых знаний получить не удалось. В колонке было очень много пропусков. Отменил в датасете этот факт. Заполнил пропуски самым популярным значением. Произвел Label Encoding"},{"metadata":{},"cell_type":"markdown","source":"## 2. 5 number of reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"def edit_number_of_reviews(input_df):\n    # all about number_of_reviews\n    \n    df = input_df.copy()\n    scaler = StandardScaler()\n    \n    # отмечу строки с пропусками, для них создам специальный столбец\n    df['Number_of_Reviews_isNAN'] = df['Number of Reviews'].isnull().astype(int)\n\n    # заполнение средними по каждому городу, но надо до каонца разобраться с синтаксисом конструкции трансформ\n    df['Number of Reviews'] = df.groupby(\"City\")['Number of Reviews'].transform(lambda x: x.fillna(x.mean()))\n    \n    \n    \n    \n        # средний ранкинг для каждого города\n    mean_per_city = df.groupby('City')['Number of Reviews'].mean()\n    df['mean_Number_of_Reviews_per_city'] = df['City'].apply(lambda x: mean_per_city[x])\n\n    #  масмимальный ранкинг для каждого города\n    max_per_city = df.groupby('City')['Number of Reviews'].max()\n    df['max_Number_of_Reviews_per_city'] = df['City'].apply(lambda x: max_per_city[x])\n\n    # стандартизация\n    df['stand_Number_of_Reviews'] = (df['Ranking'] - df['mean_Number_of_Reviews_per_city']) / df['max_Number_of_Reviews_per_city']\n    \n    \n    \n    \n    \n\n    # количество отзывов на каждого человека в городе\n    try:\n        df['reviews_per_each_person'] = df['Number of Reviews'] / df['population']\n    except:\n        pass\n\n    # среднее количество отзывов по городам\n    reviews_per_city = df.groupby(by='City')['Number of Reviews'].mean()\n    df['reviews_per_city'] = df['City'].apply(lambda x: reviews_per_city[x])\n    \n    try:\n        df['reviews_per_each_person'] = scaler.fit_transform(df[['reviews_per_each_person']])\n    except:\n        pass\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edit_city(edit_number_of_reviews(data)).sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Данные достаточно чистые, но там где были пропуски все равно был создан столбец с пометкой об этом. \n- пропуски заполнены средними значениями для каждого города, есть еще вариант заполнить пропуски нулями, но вывести новый столбец для средних значения для каждого города\n- создан столбец с количеством отзывов для каждого человека"},{"metadata":{},"cell_type":"markdown","source":"## 2.6 Ranking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# распределение параметра ранкинг. Много значений, которые даже не дотягивают до 2500 места в своем городе\nplt.rcParams['figure.figsize'] = (10,7)\ndf_train['Ranking'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# самые ресторанные города, если можно так сказать\ndata['City'].value_counts(ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Ranking'][data['City'] == 'London'].hist(bins=100)\n# не знаю почему это распределение называют нормальным. Для Лондона следующее распределение. Посмотрим на распределение других городов","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in (data['City'].value_counts())[0:10].index:\n    data['Ranking'][data['City'] == x].hist(bins=100)\nplt.show()\n\n# Здесь видно что форма распределения повторяется, но видно смещение, это связано с размерами города","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def edit_ranking(df_input):\n    # all about edit\n    df = df_input.copy()    \n    \n    # приведем значения к виду от -1 до 1\n\n    # средний ранкинг для каждого города\n    mean_per_city = df.groupby('City')['Ranking'].mean()\n    df['mean_ranking_per_city'] = df['City'].apply(lambda x: mean_per_city[x])\n\n    #  масмимальный ранкинг для каждого города\n    max_per_city = df.groupby('City')['Ranking'].max()\n    df['max_ranking_per_city'] = df['City'].apply(lambda x: max_per_city[x])\n\n    # стандартизация\n    df['stand_ranking'] = (df['Ranking'] - df['mean_ranking_per_city']) / df['max_ranking_per_city']\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edit_ranking(data).sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим обновленое распределение для тех же городов\ntest_data = edit_ranking(data)\nfor x in (test_data['City'].value_counts())[0:10].index:\n    test_data['stand_ranking'][test_data['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Признак без пропусков. Распределение одинаковое для каждого города. Можно заметить что дейсвительно смещеные было вызвано только размером города"},{"metadata":{},"cell_type":"markdown","source":"## 2.7 обработка признака Reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"def days_between_dates(dates):\n\n#     это функция высчитывает количество дней между отзывами\n        \n    if len(dates) == 0:\n        return 1800\n    if len(dates) == 1:\n        return 3600\n    dt_list = []\n    for date in dates:\n        dt = pd.to_datetime(date)\n        dt_list.append(dt)\n    return int((max(dt_list) - min(dt_list)).days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def edit_reviews(df_input):\n    # all about edit reviews\n    \n    df = df_input.copy()\n    \n\n    # пропуски найденные с помощью isna() можно заполнить пустыми списками\n    df['Reviews'] = df['Reviews'].fillna('[[], []]')\n\n    # создам признак отображающий есть ли пропуск в данном признаке\n    df['reviews_is_NAN'] = (df['Reviews'] == '[[], []]').astype(int)\n\n    # признак состоит из двух последних отзывов и даты, в которой этот отзыв оставлен \n    # для начала можно попробовать извлечь значения дат и попробовать извлечь новые знания из этого\n    df['date_of_reviews'] = df['Reviews'].str.findall('\\d+/\\d+/\\d+')\n\n    # сразу можно проверить, действительно ли содержится два отзыва\n    df['len_date_of_reviews'] = df['date_of_reviews'].apply(len)\n        \n    # видно что некоторые пользователи записывали дату своего посещения\n    # этот можно можно немного отредактировать\n    df['date_of_reviews'] = df['date_of_reviews'].apply(lambda x: x[1:] if len(x) > 2 else x)\n\n    # соответственно надо испрвить колонку len_date_of_reviews, т.к. значений = 3 больше нет\n    df['len_date_of_reviews'] = df['date_of_reviews'].apply(len)\n    \n    # создание нового признака. Количество дней между последними отзывами\n    df['days_between'] = df['date_of_reviews'].apply(days_between_dates)\n\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edit_reviews(data).head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Пробуски в признаке встречаются. Так же есть признаки, где данные заполнены наполовину. Например 1 отзыв и соответственно одна дата.\n- заполнены пропуски и создан признак с отметкой об этом\n- создан признак с количеством отзывов для конкретного рестора\n- создан признак перерыв между двумя датами. \n\nВ дальнешей можно попробовать найти оттенок отзыва. При переходе на случайную ссылку, представленной в данном датасе я заметил что оценка которая стоит в датасете не совпадает с оценкой в ресторане, поэтому не уверен что необходимо парсить данные со страницы ресторанов. Можно попробовать в дальнейшем создать новый признак такой как в какое время суток чаще всего посещают каждое заведение, но опять таки. Я не знаю насколько информация из 2020 года будет актуально для 2017"},{"metadata":{},"cell_type":"markdown","source":"## 2.8. URL_TA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['URL_TA'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['URL_TA'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Я думал что таким образом можнжо будет вычислить сетевые рестораны, но увы. Не знаю как может пригодиться этот признак, поэтому в дальнейшем я просто его удалю"},{"metadata":{},"cell_type":"markdown","source":"## 2.9 ID_TA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ID_TA'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ID_TA'].apply(lambda x: x[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data.drop(['ID_TA'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n        \n    df = df_input.copy()\n    \n    ############################## Restaurant_id   ###################################    \n    df = edit_rest_id(df)\n    \n    ############################## City   ###################################\n    df = edit_city(df)\n    df = create_dummy(df, 'City', n=31)\n    df = create_dummy(df, 'country', n=15)\n    \n    ############################## Cuisine  ###################################\n    df = edit_cuisine(df, n=30)\n        \n    ############################## Price Range  ###################################\n    df = edit_price_range(df)\n    \n    ############################## Number of Reviews  ###################################\n    df = edit_number_of_reviews(df)\n    \n    \n    ############################## Ranking  ###################################        \n    df = edit_ranking(df)\n    \n    ############################## Reviews  ###################################    \n    df = edit_reviews(df)    \n    \n    \n    df['ID_TA'] = df['ID_TA'].apply(lambda x: int(x[1:]))\n    \n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Запускаем и проверяем что получилось"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc = preproc_data(data)\ndf_preproc.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# список переменных для корреляции\nlist_for_corr = ['Restaurant_id', 'Ranking', 'Price Range', 'Number of Reviews',\n       'sample', 'Rating', 'one_or_more', 'population', 'restaurant_in_city',\n       'restaurant_per_person', 'capital', 'Cuisine_style_ISNA', 'len_cuisine',\n       'Price_Range_isNAN', 'Number_of_Reviews_isNAN',\n       'mean_Number_of_Reviews_per_city', 'max_Number_of_Reviews_per_city',\n       'stand_Number_of_Reviews', 'reviews_per_each_person',\n       'reviews_per_city', 'mean_ranking_per_city', 'max_ranking_per_city',\n       'stand_ranking', 'reviews_is_NAN', 'len_date_of_reviews',\n       'days_between']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20,8)\nax = sns.heatmap(df_preproc[list_for_corr].corr(), annot=True, fmt='.2g')\ni, k = ax.get_ylim()\nax.set_ylim(i+0.5, k-0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = ['mean_ranking_per_city', 'max_ranking_per_city', 'population', 'mean_Number_of_Reviews_per_city']\n\ndf_preproc.drop(drop_list, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns = df_preproc.select_dtypes(include='object').columns\ndf_preproc.drop(object_columns, axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # наш таргет\nX = train_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.sort_values(by=['Restaurant_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \nСам ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)\ny_pred = np.round(y_pred*2) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.sort_values(by=['Restaurant_id'])\ntest_data = test_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = model.predict(test_data)\npredict_submission = np.round(predict_submission*2) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('new_submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What's next?\nИли что делать, чтоб улучшить результат:\n* Обработать оставшиеся признаки в понятный для машины формат\n* Посмотреть, что еще можно извлечь из признаков\n* Сгенерировать новые признаки\n* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n* Подобрать состав признаков\n\nВ общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}